{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creando un RDD, usando parallelize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una forma de crear un RDD es paralelizar una lista ya existente.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import random\n",
    "import pprint\n",
    "\n",
    "sc = SparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = range(100)\n",
    "data = sc.parallelize(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wget\n",
      "  Downloading wget-3.2.zip (10 kB)\n",
      "Building wheels for collected packages: wget\n",
      "  Building wheel for wget (setup.py): started\n",
      "  Building wheel for wget (setup.py): finished with status 'done'\n",
      "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9686 sha256=8c0b39bb3913c5457505892459f909f075b69b4d777b74fc84dc24071c82761d\n",
      "  Stored in directory: c:\\users\\chris\\appdata\\local\\pip\\cache\\wheels\\bd\\a8\\c3\\3cf2c14a1837a4e04bd98631724e81f33f462d86a1d895fae0\n",
      "Successfully built wget\n",
      "Installing collected packages: wget\n",
      "Successfully installed wget-3.2\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install wget\n",
    "# !{sys.executable} -m pip install pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100% [..........................................................................] 2144903 / 2144903"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'kddcup.data_10_percent.gz'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wget\n",
    "wget.download(\"http://kdd.ics.uci.edu/databases/kddcup99/kddcup.data_10_percent.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = \"./kddcup.data_10_percent.gz\"\n",
    "raw_data = sc.textFile(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0,tcp,http,SF,181,5450,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,8,8,0.00,0.00,0.00,0.00,1.00,0.00,0.00,9,9,1.00,0.00,0.11,0.00,0.00,0.00,0.00,0.00,normal.',\n",
       " '0,tcp,http,SF,239,486,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,8,8,0.00,0.00,0.00,0.00,1.00,0.00,0.00,19,19,1.00,0.00,0.05,0.00,0.00,0.00,0.00,0.00,normal.',\n",
       " '0,tcp,http,SF,235,1337,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,8,8,0.00,0.00,0.00,0.00,1.00,0.00,0.00,29,29,1.00,0.00,0.03,0.00,0.00,0.00,0.00,0.00,normal.',\n",
       " '0,tcp,http,SF,219,1337,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,6,6,0.00,0.00,0.00,0.00,1.00,0.00,0.00,39,39,1.00,0.00,0.03,0.00,0.00,0.00,0.00,0.00,normal.',\n",
       " '0,tcp,http,SF,217,2032,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,6,6,0.00,0.00,0.00,0.00,1.00,0.00,0.00,49,49,1.00,0.00,0.02,0.00,0.00,0.00,0.00,0.00,normal.']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "494021"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformaciones y Filtros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta transformación se puede aplicar a los RDD para mantener solo elementos que satisfagan una determinada condición. Más concretamente, se evalúa una función en cada elemento en el RDD original. \n",
    "\n",
    "El nuevo RDD resultante contendrá solo aquellos elementos que hacen que la función devuelva True.\n",
    "\n",
    "Para esto utilizaremos las funciones lambdas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encontrar la frase 'normal.' en una linea de texto\n",
    "normal_raw_data = raw_data.filter(lambda x: 'normal.' in x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora podemos contar cuántos elementos tenemos en el nuevo RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existen 97278 interacciones 'normal' \n",
      "La operación Count demoro 1.217 segundos\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "\n",
    "t0 = time()\n",
    "normal_count = normal_raw_data.count()\n",
    "tt = time() - t0\n",
    "print(\"Existen {} interacciones 'normal' \".format(normal_count))\n",
    "print(\"La operación Count demoro {} segundos\".format(round(tt,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0,tcp,http,SF,181,5450,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,8,8,0.00,0.00,0.00,0.00,1.00,0.00,0.00,9,9,1.00,0.00,0.11,0.00,0.00,0.00,0.00,0.00,normal.',\n",
       " '0,tcp,http,SF,239,486,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,8,8,0.00,0.00,0.00,0.00,1.00,0.00,0.00,19,19,1.00,0.00,0.05,0.00,0.00,0.00,0.00,0.00,normal.',\n",
       " '0,tcp,http,SF,235,1337,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,8,8,0.00,0.00,0.00,0.00,1.00,0.00,0.00,29,29,1.00,0.00,0.03,0.00,0.00,0.00,0.00,0.00,normal.',\n",
       " '0,tcp,http,SF,219,1337,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,6,6,0.00,0.00,0.00,0.00,1.00,0.00,0.00,39,39,1.00,0.00,0.03,0.00,0.00,0.00,0.00,0.00,normal.',\n",
       " '0,tcp,http,SF,217,2032,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,6,6,0.00,0.00,0.00,0.00,1.00,0.00,0.00,49,49,1.00,0.00,0.02,0.00,0.00,0.00,0.00,0.00,normal.']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normal_raw_data.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe que hemos medido el tiempo transcurrido para contar los elementos en el RDD. Hemos hecho esto porque queríamos señalar que los cálculos reales (distribuidos) en Spark tienen lugar cuando ejecutamos acciones y no transformaciones. En este caso, contar es la acción que ejecutamos en el RDD. Podemos aplicar tantas transformaciones como queramos en nuestro RDD y no se realizará ningún cálculo hasta que llamemos a la primera acción que, en este caso, tarda unos segundos en completarse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Función de tranformación map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al usar la transformación del __map__ en Spark, podemos aplicar una función a cada elemento en nuestro RDD. Las lambdas de Python son especialmente expresivas para este particular."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso, queremos leer nuestro archivo de datos en formato CSV. Podemos hacer esto aplicando una función lambda a cada elemento en el RDD de la siguiente manera."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "la operacion e tomart 5 elementos se completo en 0.594 segundos\n",
      "['0', 'tcp', 'http', 'SF', '181', '5450', '0', '0', '0', '0', '0', '1', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '8', '8', '0.00', '0.00', '0.00', '0.00', '1.00', '0.00', '0.00', '9', '9', '1.00', '0.00', '0.11', '0.00', '0.00', '0.00', '0.00', '0.00', 'normal.']\n"
     ]
    }
   ],
   "source": [
    "csv_data = raw_data.map(lambda x: x.split(\",\"))\n",
    "t0 = time()\n",
    "head_rows = csv_data.take(5)\n",
    "tt = time() - t0\n",
    "print(\"la operacion e tomart 5 elementos se completo en {} segundos\".format(round(tt,3)) )\n",
    "print(head_rows[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nuevamente, todas las acciones ocurren una vez que llamamos a la primera acción de Spark (es decir, tomar en este caso). ¿Qué pasa si tomamos muchos elementos en lugar de solo los primeros?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "la operacion e tomart 100000 elementos se completo en 1.511 segundos\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "head_rows = csv_data.take(100000)\n",
    "tt = time() - t0\n",
    "print(\"la operacion e tomart 100000 elementos se completo en {} segundos\".format(round(tt,3)) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver que lleva más tiempo. La función de mapa se aplica ahora de forma distribuida a muchos elementos en el RDD, de ahí el mayor tiempo de ejecución."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usando mapa y funciones predefinidas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por supuesto, podemos usar funciones predefinidas con el mapa. Imagine que queremos tener cada elemento en el RDD como un par clave-valor donde la clave es la etiqueta (por ejemplo, normal) y el valor es la lista completa de elementos que representa la fila en el archivo con formato CSV. Podríamos proceder de la siguiente manera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# funcion parse_interacciones recibe una linea, y en su transformación devuelve \n",
    "# clave, valor, donde el primer elemento es la columnia final de la fila y el segundo \n",
    "# es el total de la fila a modo de lista\n",
    "def parse_interaction(line):\n",
    "    elems = line.split(\",\")\n",
    "    tag = elems[41]\n",
    "    return (tag, elems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('normal.',\n",
       " ['0',\n",
       "  'tcp',\n",
       "  'http',\n",
       "  'SF',\n",
       "  '181',\n",
       "  '5450',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '1',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '8',\n",
       "  '8',\n",
       "  '0.00',\n",
       "  '0.00',\n",
       "  '0.00',\n",
       "  '0.00',\n",
       "  '1.00',\n",
       "  '0.00',\n",
       "  '0.00',\n",
       "  '9',\n",
       "  '9',\n",
       "  '1.00',\n",
       "  '0.00',\n",
       "  '0.11',\n",
       "  '0.00',\n",
       "  '0.00',\n",
       "  '0.00',\n",
       "  '0.00',\n",
       "  '0.00',\n",
       "  'normal.'])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hacemos el map de RDD en base a la funcion creada, devolvemos el primer elemento \n",
    "key_csv_data = raw_data.map(parse_interaction)\n",
    "head_rows = key_csv_data.take(5)\n",
    "head_rows[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En nuestro notebook sobre cómo trabajar con pares clave-valor, utilizaremos este tipo de RDD para realizar agregaciones de datos (por ejemplo, contar por clave)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hasta ahora hemos utilizado las acciones de ___count__ y __take__ . Otra acción básica que debemos aprender es __collect__. Básicamente, tendrá __todos los elementos en el RDD en la memoria__ para que podamos trabajar con ellos. Por esta razón, debe __usarse con cuidado__, especialmente cuando se trabaja con RDD grandes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "collect en 2.623 segundos\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "all_raw_data = raw_data.collect()\n",
    "tt = time() - t0\n",
    "print (\"collect en {} segundos\".format(round(tt,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eso tomó más tiempo que cualquier otra acción que usamos antes, por supuesto. Cada nodo de trabajo de Spark que tenga un fragmento del RDD debe coordinarse para recuperar su parte y luego reducir todo junto.\n",
    "\n",
    "Como último ejemplo que combina todo lo anterior, queremos recopilar todas las interacciones normales como par clave-valor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data collected in 2.804 seconds\n",
      "There are 97278 'normal' interactions\n"
     ]
    }
   ],
   "source": [
    "# get data from file\n",
    "data_file = \"./kddcup.data_10_percent.gz\"\n",
    "raw_data = sc.textFile(data_file)\n",
    "\n",
    "# parse into key-value pairs\n",
    "key_csv_data = raw_data.map(parse_interaction)\n",
    "\n",
    "# filter normal key interactions\n",
    "normal_key_interactions = key_csv_data.filter(lambda x: x[0] == \"normal.\")\n",
    "\n",
    "# collect all\n",
    "t0 = time()\n",
    "all_normal = normal_key_interactions.collect()\n",
    "tt = time() - t0\n",
    "normal_count = len(all_normal)\n",
    "print(\"Data collected in {} seconds\".format(round(tt,3)))\n",
    "print (\"There are {} 'normal' interactions\".format(normal_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este recuento coincide con el recuento anterior para las interacciones normales. El nuevo procedimiento lleva más tiempo. Esto se debe a que recuperamos todos los datos con recopilar y luego usamos el len de Python en la lista resultante. Antes solo estábamos contando el número total de elementos en el RDD usando count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling RDDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La transformación de __muestra__ toma hasta tres parámetros. \n",
    "- Primero es si el muestreo se realiza con reemplazo o no. \n",
    "- El segundo es el tamaño de la muestra como fracción. \n",
    "- Finalmente, opcionalmente podemos proporcionar una semilla aleatoria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño de la muestra es 49493 de un total de 494021\n"
     ]
    }
   ],
   "source": [
    "raw_data_sample = raw_data.sample(False, 0.1, 1234)\n",
    "sample_size = raw_data_sample.count()\n",
    "total_size = raw_data.count()\n",
    "print (\"Tamaño de la muestra es {} de un total de {}\".format(sample_size, total_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pero el poder del muestreo como transformación proviene de hacerlo como parte de una secuencia de transformaciones adicionales. Esto se mostrará más poderoso una vez que comencemos a hacer agregaciones y operaciones de pares clave-valor, y será especialmente útil cuando usemos la biblioteca de aprendizaje automático __MLlib de Spark__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mientras tanto, imagine que queremos tener una aproximación de la proporción de lo normal. interacciones en nuestro conjunto de datos. Podríamos hacer esto contando el número total de etiquetas . Sin embargo, queremos una respuesta más rápida y no necesitamos la respuesta exacta sino solo una aproximación. Podemos hacerlo de la siguiente manera."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El ratio de interacciones 'normal' es de 0.195\n",
      "La cuenta duro 1.193 segundos\n"
     ]
    }
   ],
   "source": [
    "# Aplicaremos las transformaciones\n",
    "raw_data_sample_items = raw_data_sample.map(lambda x: x.split(\",\"))\n",
    "sample_normal_tags = raw_data_sample_items.filter(lambda x: \"normal.\" in x)\n",
    "\n",
    "# actions + time\n",
    "t0 = time()\n",
    "sample_normal_tags_count = sample_normal_tags.count()\n",
    "tt = time() - t0\n",
    "\n",
    "sample_normal_ratio = sample_normal_tags_count / float(sample_size)\n",
    "print( \"El ratio de interacciones 'normal' es de {}\".format(round(sample_normal_ratio,3)) )\n",
    "print( \"La cuenta duro {} segundos\".format(round(tt,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparemos esto con el cálculo de la relación sin muestreo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El ratio de interacciones 'normal' es de 0.197\n",
      "La cuenta duro 1.873 segundos\n"
     ]
    }
   ],
   "source": [
    "# transformations to be applied\n",
    "raw_data_items = raw_data.map(lambda x: x.split(\",\"))\n",
    "normal_tags = raw_data_items.filter(lambda x: \"normal.\" in x)\n",
    "\n",
    "# actions + time\n",
    "t0 = time()\n",
    "normal_tags_count = normal_tags.count()\n",
    "tt = time() - t0\n",
    "\n",
    "normal_ratio = normal_tags_count / float(total_size)\n",
    "print( \"El ratio de interacciones 'normal' es de {}\".format(round(normal_ratio,3)) )\n",
    "print( \"La cuenta duro {} segundos\".format(round(tt,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver una ganancia en el tiempo. Cuantas más transformaciones apliquemos después del muestreo, mayor será esta ganancia. Esto se debe a que, sin muestreo, todas las transformaciones se aplican al conjunto completo de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## función takeSample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si lo que necesitamos es tomar una muestra de datos sin procesar de nuestro RDD en la memoria local para que otras bibliotecas que no sean de Spark puedan usar, se puede usar takeSample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La sintaxis es muy similar, pero en este caso especificamos el número de elementos en lugar del tamaño de la muestra como una fracción del tamaño de datos completo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El ratio de interacciones 'normal' es de 0.197\n",
      "La cuenta duro 3.499 segundos\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "raw_data_sample = raw_data.takeSample(False, 400000, 1234)\n",
    "normal_data_sample = [x.split(\",\") for x in raw_data_sample if \"normal.\" in x]\n",
    "tt = time() - t0\n",
    "\n",
    "normal_sample_size = len(normal_data_sample)\n",
    "\n",
    "normal_ratio = normal_sample_size / 400000.0\n",
    "print( \"El ratio de interacciones 'normal' es de {}\".format(round(normal_ratio,3)) )\n",
    "print( \"La cuenta duro {} segundos\".format(round(tt,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la agregación anterior, el primer elemento acumulador mantiene la suma total, mientras que el segundo elemento mantiene el recuento. La combinación de un acumulador con un elemento RDD consiste en resumir el valor e incrementar el recuento. La combinación de dos acumuladores requiere solo una suma por pares."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sin embargo, tardó más, incluso con una muestra un poco más pequeña. La razón es que Spark acaba de distribuir la ejecución del proceso de muestreo. El filtrado y la división de los resultados se realizaron localmente en un solo nodo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obteniendo interacciones attack usando restar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con fines ilustrativos, imagine que ya tenemos nuestro RDD con interacciones sin attack (normal) de algún análisis previo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_raw_data = raw_data.filter(lambda x: \"normal.\" in x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos obtener interacciones de ataque restando las normales del RDD original sin filtrar de la siguiente manera.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "attack_raw_data = raw_data.subtract(normal_raw_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hagamos algunos recuentos para verificar nuestros resultados.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All count in 1.031 secs\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "\n",
    "# count all\n",
    "t0 = time()\n",
    "raw_data_count = raw_data.count()\n",
    "tt = time() - t0\n",
    "print( \"All count in {} secs\".format(round(tt,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal count in 1.164 secs\n"
     ]
    }
   ],
   "source": [
    "# count normal\n",
    "t0 = time()\n",
    "normal_raw_data_count = normal_raw_data.count()\n",
    "tt = time() - t0\n",
    "print (\"Normal count in {} secs\".format(round(tt,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attack count in 5.209 secs\n"
     ]
    }
   ],
   "source": [
    "# count attacks\n",
    "t0 = time()\n",
    "attack_raw_data_count = attack_raw_data.count()\n",
    "tt = time() - t0\n",
    "print( \"Attack count in {} secs\".format(round(tt,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 97278 normal interactions and 396743 attacks, from a total of 494021 interactions\n"
     ]
    }
   ],
   "source": [
    "print( \"There are {} normal interactions and {} attacks, \\\n",
    "from a total of {} interactions\".format(normal_raw_data_count,attack_raw_data_count,raw_data_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Protocolo y combinaciones de servicios usando cartesiano\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Podemos calcular el producto cartesiano entre dos RDD utilizando la transformación cartesiana. Devuelve todos los pares de elementos posibles entre dos RDD. En nuestro caso lo usaremos para generar todas las combinaciones posibles entre servicio y protocolo en nuestras interacciones de red.\n",
    "\n",
    "En primer lugar, necesitamos aislar cada colección de valores en dos RDD separados. Para eso usaremos distinto en el conjunto de datos analizados por CSV. Por la descripción del conjunto de datos, sabemos que el protocolo es la segunda columna y el servicio es la tercera (la etiqueta es la última y no la primera como aparece en la página).\n",
    "\n",
    "Primero, obtengamos __protocols__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tcp', 'udp', 'icmp']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_data = raw_data.map(lambda x: x.split(\",\"))\n",
    "protocols = csv_data.map(lambda x: x[1]).distinct()\n",
    "protocols.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Haremos lo mismo para __Service__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['http',\n",
       " 'smtp',\n",
       " 'finger',\n",
       " 'domain_u',\n",
       " 'auth',\n",
       " 'telnet',\n",
       " 'ftp',\n",
       " 'eco_i',\n",
       " 'ntp_u',\n",
       " 'ecr_i',\n",
       " 'other',\n",
       " 'private',\n",
       " 'pop_3',\n",
       " 'ftp_data',\n",
       " 'rje',\n",
       " 'time',\n",
       " 'mtp',\n",
       " 'link',\n",
       " 'remote_job',\n",
       " 'gopher',\n",
       " 'ssh',\n",
       " 'name',\n",
       " 'whois',\n",
       " 'domain',\n",
       " 'login',\n",
       " 'imap4',\n",
       " 'daytime',\n",
       " 'ctf',\n",
       " 'nntp',\n",
       " 'shell',\n",
       " 'IRC',\n",
       " 'nnsp',\n",
       " 'http_443',\n",
       " 'exec',\n",
       " 'printer',\n",
       " 'efs',\n",
       " 'courier',\n",
       " 'uucp',\n",
       " 'klogin',\n",
       " 'kshell',\n",
       " 'echo',\n",
       " 'discard',\n",
       " 'systat',\n",
       " 'supdup',\n",
       " 'iso_tsap',\n",
       " 'hostnames',\n",
       " 'csnet_ns',\n",
       " 'pop_2',\n",
       " 'sunrpc',\n",
       " 'uucp_path',\n",
       " 'netbios_ns',\n",
       " 'netbios_ssn',\n",
       " 'netbios_dgm',\n",
       " 'sql_net',\n",
       " 'vmnet',\n",
       " 'bgp',\n",
       " 'Z39_50',\n",
       " 'ldap',\n",
       " 'netstat',\n",
       " 'urh_i',\n",
       " 'X11',\n",
       " 'urp_i',\n",
       " 'pm_dump',\n",
       " 'tftp_u',\n",
       " 'tim_i',\n",
       " 'red_i']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "services = csv_data.map(lambda x: x[2]).distinct()\n",
    "services.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora hagamos un producto cartesianos con __protocol x service__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existen 198 combinaciones de protocol X service\n"
     ]
    }
   ],
   "source": [
    "product = protocols.cartesian(services).collect()\n",
    "print (\"Existen {} combinaciones de protocol X service\".format(len(product)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviamente, para RDDs tan pequeños realmente no tiene sentido usar el producto cartesiano Spark. Podríamos haber recogido perfectamente los valores después de utilizar distintos y hacer el producto cartesiano localmente. Además, las operaciones distintas y cartesianas son costosas, por lo que deben usarse con cuidado cuando los conjuntos de datos operativos son grandes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspección de la duración de la interacción por etiqueta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tanto fold como reduce toman una función como argumento que se aplica a dos elementos del RDD. La acción de plegado difiere de la reducción en que obtiene un valor cero inicial adicional que se utilizará para la llamada inicial. Este valor debe ser el elemento de identidad para la función proporcionada.\n",
    "\n",
    "Como ejemplo, imagine que queremos saber la duración total de nuestras interacciones para interacciones normales y de ataque. Podemos usar reducir de la siguiente manera."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse data\n",
    "csv_data = raw_data.map(lambda x: x.split(\",\"))\n",
    "\n",
    "# separate into different RDDs\n",
    "normal_csv_data = csv_data.filter(lambda x: x[41]==\"normal.\")\n",
    "attack_csv_data = csv_data.filter(lambda x: x[41]!=\"normal.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La función que pasamos para reducir obtiene y devuelve elementos del mismo tipo de RDD. Si queremos sumar duraciones, necesitamos extraer ese elemento en un nuevo RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_duration_data = normal_csv_data.map(lambda x: int(x[0]))\n",
    "attack_duration_data = attack_csv_data.map(lambda x: int(x[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora podemos reducir estos nuevos RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total duration for 'normal' interactions is 21075991\n",
      "Total duration for 'attack' interactions is 2626792\n"
     ]
    }
   ],
   "source": [
    "\n",
    "total_normal_duration = normal_duration_data.reduce(lambda x, y: x + y)\n",
    "total_attack_duration = attack_duration_data.reduce(lambda x, y: x + y)\n",
    "\n",
    "print (\"Total duration for 'normal' interactions is {}\".\\\n",
    "    format(total_normal_duration))\n",
    "print (\"Total duration for 'attack' interactions is {}\".\\\n",
    "    format(total_attack_duration))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tenemos un primer enfoque (y demasiado simplista) para identificar las interacciones de ataque."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Una mejor manera, usando: aggregate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "La acción agregada nos libera de la restricción de que el retorno sea del mismo tipo que el RDD en el que estamos trabajando. Al igual que con fold, proporcionamos un valor cero inicial del tipo que queremos devolver. Luego proporcionamos dos funciones. El primero se usa para combinar los elementos de nuestro RDD con el acumulador. La segunda función es necesaria para fusionar dos acumuladores. Veámoslo en acción calculando la media que hicimos antes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Promedio de 'normal' por interaccion es  216.657\n"
     ]
    }
   ],
   "source": [
    "\n",
    "normal_sum_count = normal_duration_data.aggregate(\n",
    "    (0,0), # the initial value\n",
    "    (lambda acc, value: (acc[0] + value, acc[1] + 1)), # combine value with acc\n",
    "    (lambda acc1, acc2: (acc1[0] + acc2[0], acc1[1] + acc2[1])) # combine accumulators\n",
    ")\n",
    "\n",
    "print (\"Promedio de 'normal' por interaccion es  {}\".\\\n",
    "    format(round(normal_sum_count[0]/float(normal_sum_count[1]),3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la agregación anterior, el primer elemento acumulador mantiene la suma total, mientras que el segundo elemento mantiene el recuento. La combinación de un acumulador con un elemento RDD consiste en resumir el valor e incrementar el recuento. La combinación de dos acumuladores requiere solo una suma por pares.\n",
    "\n",
    "Podemos hacer lo mismo con las interacciones de tipo ataque."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Promedio de 'attack' por interaccion es 6.621\n"
     ]
    }
   ],
   "source": [
    "attack_sum_count = attack_duration_data.aggregate(\n",
    "    (0,0), # the initial value\n",
    "    (lambda acc, value: (acc[0] + value, acc[1] + 1)), # combine value with acc\n",
    "    (lambda acc1, acc2: (acc1[0] + acc2[0], acc1[1] + acc2[1])) # combine accumulators\n",
    ")\n",
    "\n",
    "print (\"Promedio de 'attack' por interaccion es {}\".\\\n",
    "    format(round(attack_sum_count[0]/float(attack_sum_count[1]),3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agregaciones de datos con RDD de pares clave / valor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos usar todas las transformaciones y acciones disponibles para RDD normales con RDD de pares clave / valor. Solo necesitamos hacer que las funciones funcionen con elementos de par. Además, Spark proporciona funciones específicas para trabajar con RDD que contienen elementos de par. Son muy similares a los disponibles para los RDD generales.\n",
    "\n",
    "Por ejemplo, tenemos una transformación reduceByKey que podemos usar de la siguiente manera para calcular la duración total de cada tipo de interacción de red.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_value_data = csv_data.map(lambda x: (x[41], x)) # x[41] contains the network interaction tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('normal.', 21075991.0),\n",
       " ('buffer_overflow.', 2751.0),\n",
       " ('loadmodule.', 326.0),\n",
       " ('perl.', 124.0),\n",
       " ('neptune.', 0.0),\n",
       " ('smurf.', 0.0),\n",
       " ('guess_passwd.', 144.0),\n",
       " ('pod.', 0.0),\n",
       " ('teardrop.', 0.0),\n",
       " ('portsweep.', 1991911.0),\n",
       " ('ipsweep.', 43.0),\n",
       " ('land.', 0.0),\n",
       " ('ftp_write.', 259.0),\n",
       " ('back.', 284.0),\n",
       " ('imap.', 72.0),\n",
       " ('satan.', 64.0),\n",
       " ('phf.', 18.0),\n",
       " ('nmap.', 0.0),\n",
       " ('multihop.', 1288.0),\n",
       " ('warezmaster.', 301.0),\n",
       " ('warezclient.', 627563.0),\n",
       " ('spy.', 636.0),\n",
       " ('rootkit.', 1008.0)]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key_value_duration = csv_data.map(lambda x: (x[41], float(x[0]))) \n",
    "durations_by_key = key_value_duration.reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "durations_by_key.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tenemos una acción de conteo específica para pares clave / valor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'normal.': 97278,\n",
       "             'buffer_overflow.': 30,\n",
       "             'loadmodule.': 9,\n",
       "             'perl.': 3,\n",
       "             'neptune.': 107201,\n",
       "             'smurf.': 280790,\n",
       "             'guess_passwd.': 53,\n",
       "             'pod.': 264,\n",
       "             'teardrop.': 979,\n",
       "             'portsweep.': 1040,\n",
       "             'ipsweep.': 1247,\n",
       "             'land.': 21,\n",
       "             'ftp_write.': 8,\n",
       "             'back.': 2203,\n",
       "             'imap.': 12,\n",
       "             'satan.': 1589,\n",
       "             'phf.': 4,\n",
       "             'nmap.': 231,\n",
       "             'multihop.': 7,\n",
       "             'warezmaster.': 20,\n",
       "             'warezclient.': 1020,\n",
       "             'spy.': 2,\n",
       "             'rootkit.': 10})"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts_by_key = key_value_data.countByKey()\n",
    "counts_by_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usando combineByKey"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta es la más general de las funciones de agregación por clave. La mayoría de los otros combinadores por tecla se implementan usándolo. Podemos considerarlo como el equivalente agregado, ya que permite al usuario devolver valores que no son del mismo tipo que nuestros datos de entrada.\n",
    "\n",
    "Por ejemplo, podemos usarlo para calcular las duraciones promedio por tipo de la siguiente manera."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'normal.': (21075991.0, 97278),\n",
       " 'buffer_overflow.': (2751.0, 30),\n",
       " 'loadmodule.': (326.0, 9),\n",
       " 'perl.': (124.0, 3),\n",
       " 'neptune.': (0.0, 107201),\n",
       " 'smurf.': (0.0, 280790),\n",
       " 'guess_passwd.': (144.0, 53),\n",
       " 'pod.': (0.0, 264),\n",
       " 'teardrop.': (0.0, 979),\n",
       " 'portsweep.': (1991911.0, 1040),\n",
       " 'ipsweep.': (43.0, 1247),\n",
       " 'land.': (0.0, 21),\n",
       " 'ftp_write.': (259.0, 8),\n",
       " 'back.': (284.0, 2203),\n",
       " 'imap.': (72.0, 12),\n",
       " 'satan.': (64.0, 1589),\n",
       " 'phf.': (18.0, 4),\n",
       " 'nmap.': (0.0, 231),\n",
       " 'multihop.': (1288.0, 7),\n",
       " 'warezmaster.': (301.0, 20),\n",
       " 'warezclient.': (627563.0, 1020),\n",
       " 'spy.': (636.0, 2),\n",
       " 'rootkit.': (1008.0, 10)}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_counts = key_value_duration.combineByKey(\n",
    "    (lambda x: (x, 1)), # the initial value, with value x and count 1\n",
    "    (lambda acc, value: (acc[0]+value, acc[1]+1)), # how to combine a pair value with the accumulator: sum value, and increment count\n",
    "    (lambda acc1, acc2: (acc1[0]+acc2[0], acc1[1]+acc2[1])) # combine accumulators\n",
    ")\n",
    "\n",
    "sum_counts.collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('normal.', (21075991.0, 97278)),\n",
       " ('buffer_overflow.', (2751.0, 30)),\n",
       " ('loadmodule.', (326.0, 9)),\n",
       " ('perl.', (124.0, 3)),\n",
       " ('neptune.', (0.0, 107201))]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_counts.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "portsweep. 1915.299\n",
      "warezclient. 615.258\n",
      "spy. 318.0\n",
      "normal. 216.657\n",
      "multihop. 184.0\n",
      "rootkit. 100.8\n",
      "buffer_overflow. 91.7\n",
      "perl. 41.333\n",
      "loadmodule. 36.222\n",
      "ftp_write. 32.375\n",
      "warezmaster. 15.05\n",
      "imap. 6.0\n",
      "phf. 4.5\n",
      "guess_passwd. 2.717\n",
      "back. 0.129\n",
      "satan. 0.04\n",
      "ipsweep. 0.034\n",
      "neptune. 0.0\n",
      "smurf. 0.0\n",
      "pod. 0.0\n",
      "teardrop. 0.0\n",
      "land. 0.0\n",
      "nmap. 0.0\n"
     ]
    }
   ],
   "source": [
    "duration_means_by_type = sum_counts.map(lambda x: (x[0], round(x[1][0]/x[1][1],3))).collectAsMap()\n",
    "\n",
    "# Print them sorted\n",
    "for tag in sorted(duration_means_by_type, key=duration_means_by_type.get, reverse=True):\n",
    "    print( tag, duration_means_by_type[tag])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
